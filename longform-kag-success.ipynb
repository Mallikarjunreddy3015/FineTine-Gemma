{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q -U datasets\n!pip install -q -U transformers\n# !pip install -q -U rouge_score","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","colab":{"base_uri":"https://localhost:8080/","height":321},"id":"bChQJdfzwJ6L","outputId":"316af55e-f296-4402-8232-ca841d03fbd6","execution":{"iopub.status.busy":"2024-03-12T16:04:08.926880Z","iopub.execute_input":"2024-03-12T16:04:08.927234Z","iopub.status.idle":"2024-03-12T16:05:01.507984Z","shell.execute_reply.started":"2024-03-12T16:04:08.927206Z","shell.execute_reply":"2024-03-12T16:05:01.506521Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 23.8.0 requires cubinlinker, which is not installed.\ncudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 23.8.0 requires ptxcompiler, which is not installed.\ncuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 15.0.1 which is incompatible.\nbeatrix-jupyterlab 2023.128.151533 requires jupyterlab~=3.6.0, but you have jupyterlab 4.1.2 which is incompatible.\ncudf 23.8.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.3.0 which is incompatible.\ncudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ncudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\ncudf 23.8.0 requires pyarrow==11.*, but you have pyarrow 15.0.1 which is incompatible.\ncuml 23.8.0 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\ndask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\ndask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"!pip install accelerate -U -q\n!pip install transformers[torch] -U -q","metadata":{"id":"DDYSNF8Lx8bl","execution":{"iopub.status.busy":"2024-03-12T16:05:01.510599Z","iopub.execute_input":"2024-03-12T16:05:01.511032Z","iopub.status.idle":"2024-03-12T16:05:28.348605Z","shell.execute_reply.started":"2024-03-12T16:05:01.510991Z","shell.execute_reply":"2024-03-12T16:05:28.347368Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"HUGGINGFACE_TOKEN\")\nsecret_value_1 = user_secrets.get_secret(\"WANDB_API_KEY\")","metadata":{"execution":{"iopub.status.busy":"2024-03-12T16:08:53.855838Z","iopub.execute_input":"2024-03-12T16:08:53.856200Z","iopub.status.idle":"2024-03-12T16:08:54.235232Z","shell.execute_reply.started":"2024-03-12T16:08:53.856170Z","shell.execute_reply":"2024-03-12T16:08:54.234373Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"import torch \nimport os\nfrom huggingface_hub import login\nimport wandb\nwandb.login(key=secret_value_1)\nlogin(token=secret_value_0)","metadata":{"execution":{"iopub.status.busy":"2024-03-12T16:08:55.320065Z","iopub.execute_input":"2024-03-12T16:08:55.320737Z","iopub.status.idle":"2024-03-12T16:09:01.746918Z","shell.execute_reply.started":"2024-03-12T16:08:55.320703Z","shell.execute_reply":"2024-03-12T16:09:01.745791Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"name":"stdout","text":"Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\nToken is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"code","source":"from datasets import load_dataset, load_metric\n","metadata":{"id":"Nx1sEsTQwJ6P","execution":{"iopub.status.busy":"2024-03-12T16:05:28.350219Z","iopub.execute_input":"2024-03-12T16:05:28.350648Z","iopub.status.idle":"2024-03-12T16:05:29.858666Z","shell.execute_reply.started":"2024-03-12T16:05:28.350608Z","shell.execute_reply":"2024-03-12T16:05:29.857555Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"from datasets import Dataset\ndataset_train = load_dataset(\"zqz979/meta-review\", split=\"train\")\ndataset_eval = load_dataset(\"zqz979/meta-review\", split=\"validation\")\ndataset_test = load_dataset(\"zqz979/meta-review\", split=\"test\")\n\ndef preprocess_dataset(dataset, input_column):\n  def update_input(example):\n    if example[input_column]:  # Check if empty\n        if isinstance(example[input_column], str):  # Check if it's a string\n            word_count = len(example[\"Output\"].split())\n            example[input_column] = f'summarize the following data in {word_count} words : {example[input_column]}'\n        else:\n            example[input_column] = \"Data removed (not a string)\"  # Placeholder for non-string data\n            example[\"Output\"] = \"Output removed (input was not a string)\"\n    else:\n        example[input_column] = \"Data removed (empty)\"  # Placeholder for empty data\n        example[\"Output\"] = \"Output removed (input was empty)\"\n    return example\n\n  return dataset.map(update_input)\n\n\nfiltered_dataset_train =preprocess_dataset(dataset_train, input_column=\"Input\")\nfiltered_dataset_eval = preprocess_dataset(dataset_eval, input_column=\"Input\")\nfiltered_dataset_test = preprocess_dataset(dataset_test, input_column=\"Input\")","metadata":{"id":"JMLfuQgpwJ6Q","execution":{"iopub.status.busy":"2024-03-12T16:05:29.860700Z","iopub.execute_input":"2024-03-12T16:05:29.861186Z","iopub.status.idle":"2024-03-12T16:05:48.899140Z","shell.execute_reply.started":"2024-03-12T16:05:29.861159Z","shell.execute_reply":"2024-03-12T16:05:48.898142Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/1.65k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"739944d3b230457f9ea1c3940b83e99f"}},"metadata":{}},{"name":"stderr","text":"Downloading data: 100%|██████████| 83.9M/83.9M [00:06<00:00, 13.9MB/s]\nDownloading data: 100%|██████████| 18.0M/18.0M [00:01<00:00, 9.68MB/s]\nDownloading data: 100%|██████████| 18.0M/18.0M [00:01<00:00, 12.6MB/s]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0898002fed0b44b8aeca7cfb748acf15"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc330756a91e4017855b7da846782825"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cdfe322eb7a245e59a82314f3a77f81e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/7692 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ac3a77b3a144eef8a085e5c5de82c30"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1648 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"13afe4d2890048bfafc1152f78989dd4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1649 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c6bc4c8c468f4bf0b81de4b7ffa80a43"}},"metadata":{}}]},{"cell_type":"code","source":"from transformers import AutoTokenizer","metadata":{"id":"lDJ8n2TEwJ6S","execution":{"iopub.status.busy":"2024-03-12T16:09:04.713929Z","iopub.execute_input":"2024-03-12T16:09:04.715083Z","iopub.status.idle":"2024-03-12T16:09:06.274667Z","shell.execute_reply.started":"2024-03-12T16:09:04.715035Z","shell.execute_reply":"2024-03-12T16:09:06.273703Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"allenai/led-base-16384\")","metadata":{"id":"mgdqngLmwJ6S","execution":{"iopub.status.busy":"2024-03-12T16:09:06.602335Z","iopub.execute_input":"2024-03-12T16:09:06.602984Z","iopub.status.idle":"2024-03-12T16:09:08.081172Z","shell.execute_reply.started":"2024-03-12T16:09:06.602952Z","shell.execute_reply":"2024-03-12T16:09:08.080205Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/27.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"25c93751bf074d4ab04c39a01ca72281"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.09k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e0a7c3d37f340aabff4b0486324e176"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df1e2999fca54a1c93ca855e64f542cf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7257aa010b3c4b64876ee31b6baaac86"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/772 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"39326a72d02d44f698c33707b7c95fe4"}},"metadata":{}}]},{"cell_type":"code","source":"max_input_length = 8192\nmax_output_length = 512\nbatch_size = 2","metadata":{"id":"jptB4k67wJ6T","execution":{"iopub.status.busy":"2024-03-12T16:09:09.976095Z","iopub.execute_input":"2024-03-12T16:09:09.976867Z","iopub.status.idle":"2024-03-12T16:09:09.981056Z","shell.execute_reply.started":"2024-03-12T16:09:09.976828Z","shell.execute_reply":"2024-03-12T16:09:09.979983Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def process_data_to_model_inputs(batch):\n    # tokenize the inputs and labels\n    inputs = tokenizer(\n        batch[\"Input\"],\n        padding=\"max_length\",\n        truncation=True,\n        max_length=max_input_length,\n    )\n    outputs = tokenizer(\n        batch[\"Output\"],\n        padding=\"max_length\",\n        truncation=True,\n        max_length=max_output_length,\n    )\n\n    batch[\"input_ids\"] = inputs.input_ids\n    batch[\"attention_mask\"] = inputs.attention_mask\n\n    # create 0 global_attention_mask lists\n    batch[\"global_attention_mask\"] = len(batch[\"input_ids\"]) * [\n        [0 for _ in range(len(batch[\"input_ids\"][0]))]\n    ]\n\n    # since above lists are references, the following line changes the 0 index for all samples\n    batch[\"global_attention_mask\"][0][0] = 1\n    batch[\"labels\"] = outputs.input_ids\n\n    # We have to make sure that the PAD token is ignored\n    batch[\"labels\"] = [\n        [-100 if token == tokenizer.pad_token_id else token for token in labels]\n        for labels in batch[\"labels\"]\n    ]\n\n    return batch","metadata":{"id":"AQfV4eQSwJ6U","execution":{"iopub.status.busy":"2024-03-12T16:09:17.740299Z","iopub.execute_input":"2024-03-12T16:09:17.740769Z","iopub.status.idle":"2024-03-12T16:09:17.751305Z","shell.execute_reply.started":"2024-03-12T16:09:17.740727Z","shell.execute_reply":"2024-03-12T16:09:17.750324Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"train_dataset = filtered_dataset_train.map(\n    process_data_to_model_inputs,\n    batched=True,\n    batch_size=batch_size,\n    remove_columns=[\"Input\", \"Output\"],\n)\n\n\nval_dataset = filtered_dataset_eval.map(\n    process_data_to_model_inputs,\n    batched=True,\n    batch_size=batch_size,\n    remove_columns=[\"Input\", \"Output\"],\n)\n\n","metadata":{"id":"O5qFtlaTwJ6V","execution":{"iopub.status.busy":"2024-03-12T16:09:24.386889Z","iopub.execute_input":"2024-03-12T16:09:24.387700Z","iopub.status.idle":"2024-03-12T16:11:19.470346Z","shell.execute_reply.started":"2024-03-12T16:09:24.387662Z","shell.execute_reply":"2024-03-12T16:11:19.469316Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/7692 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eab768ff2e2a4fec8784a88b19998ab5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1648 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c4b9cd72ce247f89d3856e3d0bf01cd"}},"metadata":{}}]},{"cell_type":"code","source":"from transformers import AutoModelForSeq2SeqLM","metadata":{"id":"sIkCESD5wJ6V","execution":{"iopub.status.busy":"2024-03-12T16:12:05.324834Z","iopub.execute_input":"2024-03-12T16:12:05.325499Z","iopub.status.idle":"2024-03-12T16:12:05.344555Z","shell.execute_reply.started":"2024-03-12T16:12:05.325464Z","shell.execute_reply":"2024-03-12T16:12:05.343764Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"led = AutoModelForSeq2SeqLM.from_pretrained(\"allenai/led-base-16384\", gradient_checkpointing=True, use_cache=False)","metadata":{"id":"aZoVLDnqwJ6W","execution":{"iopub.status.busy":"2024-03-12T16:12:07.009057Z","iopub.execute_input":"2024-03-12T16:12:07.009473Z","iopub.status.idle":"2024-03-12T16:12:12.100666Z","shell.execute_reply.started":"2024-03-12T16:12:07.009441Z","shell.execute_reply":"2024-03-12T16:12:12.099598Z"},"trusted":true},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/648M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b88c00bc16e44af18172272c1c0000f4"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/168 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"14bb1315d26840428f0fc343d1b740b0"}},"metadata":{}}]},{"cell_type":"code","source":"led.config.num_beams = 2\nled.config.max_length = 512\nled.config.min_length = 100\nled.config.length_penalty = 2.0\nled.config.early_stopping = True\nled.config.no_repeat_ngram_size = 3","metadata":{"id":"TbtGUfx5wJ6W","execution":{"iopub.status.busy":"2024-03-12T16:12:12.102749Z","iopub.execute_input":"2024-03-12T16:12:12.103853Z","iopub.status.idle":"2024-03-12T16:12:12.109605Z","shell.execute_reply.started":"2024-03-12T16:12:12.103811Z","shell.execute_reply":"2024-03-12T16:12:12.108635Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# rouge = load_metric(\"rouge\")\n","metadata":{"execution":{"iopub.status.busy":"2024-03-12T12:54:12.387324Z","iopub.execute_input":"2024-03-12T12:54:12.388262Z","iopub.status.idle":"2024-03-12T12:54:13.970177Z","shell.execute_reply.started":"2024-03-12T12:54:12.388228Z","shell.execute_reply":"2024-03-12T12:54:13.969157Z"},"id":"zTLfWieIwJ6W","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def compute_metrics(pred):\n    labels_ids = pred.label_ids\n    pred_ids = pred.predictions\n\n    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n    labels_ids[labels_ids == -100] = tokenizer.pad_token_id\n    label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n\n    rouge_output = rouge.compute(\n        predictions=pred_str, references=label_str, rouge_types=[\"rouge2\"]\n    )[\"rouge2\"].mid\n\n    return {\n        \"rouge2_precision\": round(rouge_output.precision, 4),\n        \"rouge2_recall\": round(rouge_output.recall, 4),\n        \"rouge2_fmeasure\": round(rouge_output.fmeasure, 4),\n    }","metadata":{"id":"kCcKDEftwJ6X","execution":{"iopub.status.busy":"2024-03-12T16:12:12.423677Z","iopub.execute_input":"2024-03-12T16:12:12.424420Z","iopub.status.idle":"2024-03-12T16:12:12.430755Z","shell.execute_reply.started":"2024-03-12T16:12:12.424388Z","shell.execute_reply":"2024-03-12T16:12:12.429750Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments","metadata":{"id":"bcnf-qs2wJ6X","execution":{"iopub.status.busy":"2024-03-12T16:12:14.738830Z","iopub.execute_input":"2024-03-12T16:12:14.739214Z","iopub.status.idle":"2024-03-12T16:12:25.981745Z","shell.execute_reply.started":"2024-03-12T16:12:14.739185Z","shell.execute_reply":"2024-03-12T16:12:25.980764Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"2024-03-12 16:12:17.037977: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-03-12 16:12:17.038113: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-03-12 16:12:17.164429: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"# enable fp16 apex training\ntraining_args = Seq2SeqTrainingArguments(\n    predict_with_generate=True,\n    evaluation_strategy=\"steps\",\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size,\n    fp16=True,\n    output_dir=\"./\",\n    logging_steps=5,\n    eval_steps=100,\n    save_steps=100,\n    save_total_limit=2,\n    gradient_accumulation_steps=4,\n    num_train_epochs=0.1,\n)","metadata":{"id":"vkC9gdRJwJ6X","execution":{"iopub.status.busy":"2024-03-12T16:12:25.983644Z","iopub.execute_input":"2024-03-12T16:12:25.984642Z","iopub.status.idle":"2024-03-12T16:12:26.051838Z","shell.execute_reply.started":"2024-03-12T16:12:25.984607Z","shell.execute_reply":"2024-03-12T16:12:26.050651Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"trainer = Seq2SeqTrainer(\n    model=led,\n    tokenizer=tokenizer,\n    args=training_args,\n    # compute_metrics=compute_metrics,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n)","metadata":{"id":"6Ahei3-HwJ6Y","execution":{"iopub.status.busy":"2024-03-12T16:12:26.053286Z","iopub.execute_input":"2024-03-12T16:12:26.054017Z","iopub.status.idle":"2024-03-12T16:12:26.450789Z","shell.execute_reply.started":"2024-03-12T16:12:26.053977Z","shell.execute_reply":"2024-03-12T16:12:26.449625Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer.train()","metadata":{"id":"a-GSnSpMwJ6Y","execution":{"iopub.status.busy":"2024-03-12T16:13:48.388398Z","iopub.execute_input":"2024-03-12T16:13:48.388777Z","iopub.status.idle":"2024-03-12T16:39:21.588728Z","shell.execute_reply.started":"2024-03-12T16:13:48.388746Z","shell.execute_reply":"2024-03-12T16:39:21.587725Z"},"trusted":true},"execution_count":22,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='97' max='97' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [97/97 25:16, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=97, training_loss=3.2943158936254755, metrics={'train_runtime': 1532.712, 'train_samples_per_second': 0.502, 'train_steps_per_second': 0.063, 'total_flos': 4298726097027072.0, 'train_loss': 3.2943158936254755, 'epoch': 0.1})"},"metadata":{}}]},{"cell_type":"code","source":"led.save_pretrained(\"colab_model\",safe_serialization=True)\ntokenizer.save_pretrained(\"colab_model\")\n","metadata":{"id":"zXJgzPRXBeJi","execution":{"iopub.status.busy":"2024-03-12T16:39:21.590642Z","iopub.execute_input":"2024-03-12T16:39:21.591046Z","iopub.status.idle":"2024-03-12T16:39:22.934749Z","shell.execute_reply.started":"2024-03-12T16:39:21.591011Z","shell.execute_reply":"2024-03-12T16:39:22.933611Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stderr","text":"Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 512, 'min_length': 100, 'early_stopping': True, 'num_beams': 2, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}\nYour generation config was originally created from the model config, but the model config has changed since then. Unless you pass the `generation_config` argument to this model's `generate` calls, they will revert to the legacy behavior where the base `generate` parameterization is loaded from the model config instead. To avoid this behavior and this warning, we recommend you to overwrite the generation config model attribute before calling the model's `save_pretrained`, preferably also removing any generation kwargs from the model config. This warning will be raised to an exception in v4.41.\n","output_type":"stream"},{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"('colab_model/tokenizer_config.json',\n 'colab_model/special_tokens_map.json',\n 'colab_model/vocab.json',\n 'colab_model/merges.txt',\n 'colab_model/added_tokens.json',\n 'colab_model/tokenizer.json')"},"metadata":{}}]},{"cell_type":"code","source":"/kaggle/working/colab_model/config.json","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"led.push_to_hub(\"my-awesome-model\", config=\"/kaggle/working/colab_model/config.json\")","metadata":{"id":"hIgty5ZCLzjc","execution":{"iopub.status.busy":"2024-03-12T16:44:59.820160Z","iopub.execute_input":"2024-03-12T16:44:59.820598Z","iopub.status.idle":"2024-03-12T16:45:23.397119Z","shell.execute_reply.started":"2024-03-12T16:44:59.820565Z","shell.execute_reply":"2024-03-12T16:45:23.396136Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stderr","text":"Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 512, 'min_length': 100, 'early_stopping': True, 'num_beams': 2, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}\nYour generation config was originally created from the model config, but the model config has changed since then. Unless you pass the `generation_config` argument to this model's `generate` calls, they will revert to the legacy behavior where the base `generate` parameterization is loaded from the model config instead. To avoid this behavior and this warning, we recommend you to overwrite the generation config model attribute before calling the model's `save_pretrained`, preferably also removing any generation kwargs from the model config. This warning will be raised to an exception in v4.41.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/648M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4428a7d6a62b4161ab3032f86b5c75c6"}},"metadata":{}},{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/Animus-09/my-awesome-model/commit/43d1e25572e8d0db0b06a28530533abee84c141a', commit_message='Upload LEDForConditionalGeneration', commit_description='', oid='43d1e25572e8d0db0b06a28530533abee84c141a', pr_url=None, pr_revision=None, pr_num=None)"},"metadata":{}}]},{"cell_type":"code","source":"  encodeds = tokenizer(filtered_dataset_test[\"Input\"][0], return_tensors=\"pt\")\n\n  model_inputs = encodeds.to(\"cuda:0\")\n\n\n  generated_ids = led.generate(**model_inputs)\n  decoded = tokenizer.decode(generated_ids[0], skip_special_tokens=True)","metadata":{"execution":{"iopub.status.busy":"2024-03-12T16:50:12.203084Z","iopub.execute_input":"2024-03-12T16:50:12.204020Z","iopub.status.idle":"2024-03-12T16:50:15.490745Z","shell.execute_reply.started":"2024-03-12T16:50:12.203988Z","shell.execute_reply":"2024-03-12T16:50:15.489614Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1339: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n  warnings.warn(\nInput ids are automatically padded from 2697 to 3072 to be a multiple of `config.attention_window`: 1024\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"print(\"human summary =====\",filtered_dataset_test[\"Output\"][0])\nprint(\"---------------------\")\nprint(\"finetuned model =====\" ,decoded)","metadata":{"execution":{"iopub.status.busy":"2024-03-12T16:51:46.165926Z","iopub.execute_input":"2024-03-12T16:51:46.166326Z","iopub.status.idle":"2024-03-12T16:51:46.181088Z","shell.execute_reply.started":"2024-03-12T16:51:46.166293Z","shell.execute_reply":"2024-03-12T16:51:46.179946Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"human summary ===== The paper present results using syntactic information (primarily through constituency trees) on the task of recognizing argument discourse units. No reviewer recommends acceptance of the paper: - The empirical results appear strong, though the reviewers raise questions about some of the experimental choices.  - The writing is unclear and reviewers point out many missing or incorrect references in the bibliography. - There is little methodological novelty - known techniques are applied to a topic that has not been studied much. Overall, the area chair agrees with the reviewers that this work does not yet meet the bar for ICLR.\n---------------------\nfinetuned model ===== This paper presents a new approach for addressing argument discourse unit (ADU) detection, by incorporating constituency trees in the model.  The paper presents an interesting approach to the problem of ADU recognition.   The reviewers agree that the paper is interesting, but the paper does not have a strong literature review or presentation. The reviewers also found the paper to be lacking in novelty and novelty. The authors should have addressed the concerns raised by the reviewers, and the authors should be able to improve the paper.\n","output_type":"stream"}]}]}